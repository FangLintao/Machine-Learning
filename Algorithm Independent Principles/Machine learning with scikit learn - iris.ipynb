{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from matplotlib import style\n",
    "import matplotlib.pyplot as plt\n",
    "style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save \"bunch\" object containing iris dataset and its attributes\n",
    "iris = load_iris()\n",
    "print(iris.keys())\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "test = [[3,5,4,2],[5,4,3,2]]\n",
    "print (X.shape)\n",
    "print (y.shape)\n",
    "\n",
    "plt.scatter(X[:50,0],X[:50,1],marker = 'o',cmap='b',s=50,linewidth=1)\n",
    "plt.scatter(X[50:100,0],X[50:100,1],marker = 'o',cmap='k',s=50,linewidth=1)\n",
    "plt.scatter(X[100:150,0],X[100:150,1],marker = 'o',cmap='r',s=50,linewidth=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Evaluation Procedure: Train/Valid split\n",
    "###### Note: Valid is usually called Test, but it doesn't really mean Test data in testing stage)\n",
    "###### train_test_split( )\n",
    "-> Provides a high-variance estimate of out-of-sample accuracy  \n",
    "-> K-fold cross-validation overcomes this limiation  \n",
    "-> train_test_split() is still used because of its flexibility and speed  \n",
    "###### Validation accuracy: a better estimate than training accuracy of out-of-sample performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import  train_test_split\n",
    "# 40% for validation set & 60% for Training set\n",
    "x_train,x_valid,y_train,y_valid = train_test_split(X,y,test_size=0.4)\n",
    "print(\" X training: \", x_train.shape)\n",
    "print(\" X valid: \", x_valid.shape)\n",
    "print(\" Y training: \", y_train.shape)\n",
    "print(\" Y valid: \", y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Logreg = LogisticRegression()\n",
    "Logreg.fit(x_train,y_train)\n",
    "y_labels = Logreg.predict(x_valid)\n",
    "print(metrics.accuracy_score(y_valid,y_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"Dataset/Advertising.csv\",index_col=0)\n",
    "# Fetures: TV, Radio, Newspaper\n",
    "# Responses: Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# allow plots to appear within the notebook\n",
    "%matplotlib inline\n",
    "# showing relationships between Tv, Radio, Newspaper and Sales respectively\n",
    "sns.pairplot(data, x_vars=['TV','Radio','Newspaper'],y_vars='Sales', size=7,kind='reg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['TV', 'Radio', 'Newspaper']\n",
    "X = data[feature_cols]\n",
    "y = data.Sales\n",
    "x_train,x_valid, y_train,y_valid = train_test_split(X,y,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(x_train, y_train)\n",
    "print(linreg.intercept_)\n",
    "print(linreg.coef_)\n",
    "for i in zip(feature_cols, linreg.coef_):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model evaluation metrics for regression\n",
    "Note: Evaluation metrics for Classification probelms, such as accuracy, are not useful for Regression problems!!!!!!  \n",
    "-> Evaluation metrics should be designed for comparing contnuous values  \n",
    "1. Mean Absolute Error (MAE): the mean of the absolute value of the errors $$\\frac{1}{n}\\sum_{i=1}^n{|{y_i-\\hat{y}_i}|}$$  \n",
    "2. Mean Square Error (MSE): the mean of the square value of the errors $$\\frac{1}{n}\\sum_{i=1}^n{(y_i-\\hat{y}_i)}^2$$  \n",
    "3. Root Mean Absolute Error (MAE): the mean of the square root value of teh errors $$\\sqrt{\\frac{1}{n}\\sum_{i=1}^n{(y_i-\\hat{y}_i)}^2}$$  \n",
    "##### Comparison\n",
    "-> MAE: the easiest to understand because it averages error  \n",
    "-> MSE: more popular than MAE because MSE \"punishes\" larger errors  \n",
    "-> RMSE: even more popular than MSE, because RMSE is interpretable in the y units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = linreg.predict(x_valid)\n",
    "print(metrics.mean_squared_error(y_labels,y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Feature Selection\n",
    "Note: need to consider which coloumns will have less effects on final prediction, so remove these columns and test again  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Selecting best model in scikit learn using \" Cross Validation\" \n",
    "###### by setting different values for random_state in \"train_test_split()\",  this will affect the accuracy in validation stage\n",
    "Note: this is high variance estimor -> potential problem  \n",
    "-> Solution: by setting a group of random_state for \"train_test_split()\", and average final accuracy results together -> essence of cross-validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example to show the potential problem\n",
    "import numpy as np\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "accuracy = []\n",
    "for i in range(100):\n",
    "    x_train,x_valid,y_train,y_valid = train_test_split(X,y,random_state=i)\n",
    "    knn = KNN(n_neighbors=5)\n",
    "    knn.fit(x_train,y_train)\n",
    "    y_predict = knn.predict(x_valid)\n",
    "    accuracy .append(metrics.accuracy_score(y_predict,y_valid) )\n",
    "plt.plot(np.arange(100),accuracy)\n",
    "plt.xlabel(\" random_state \")\n",
    "plt.ylabel(\" accuarcy \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.1 K-fold Cross Validation\n",
    "![image](K.png)\n",
    "###### 4.2 Comparison K-fold Cross Validation and Train/Test Split\n",
    "-> Advantages of cross_validation:  \n",
    ". More accurate estimate of out of sample accuracy  \n",
    ". More \"efficient\" use of data -- [ every observation is used for both training and validation stage ]  \n",
    "-> Advantages of Train/Test split:  \n",
    ". Run K times faster than K-fold cross-validation  \n",
    ". Simpler to examine the detailed results of testing process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "X = np.arange(25)\n",
    "kf = KFold(n_splits=5,shuffle=False)\n",
    "for train_index , test_index in kf.split(X):\n",
    "    print('train_index:%s , test_index: %s ' %(train_index,test_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.3 Cross Validation Recommendation\n",
    "1. K can be any number, but k=10 is greatly recommended  \n",
    "2. For Classification problem, stratified sampling[分层抽样] is recommended for creating the folds  \n",
    ". Each response class should be represented with equal proportions in each of the k folds  \n",
    ". Scikit-learn's cross_val_score fucntion does this by default\n",
    "###### \"cross_val_score()\"  working principle is \" Steps for K-fold cross-validation\"\n",
    "######  relationship between \" cross_val_score()\" and \" KFold()\"\n",
    "\"cross_val_score()\" cover KFold(), and more, cross_val_score() automatically send data to model knn,for example, and get scores as results\n",
    "![image](2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.3.1 Cross-Validation example: parameter tuning\n",
    "Note: Goal: select the best tuning parameters (aka \" hyperparameters\") for KNN on the iris dataset  \n",
    "-> result: k=20 offers better scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "# 10 fold cross validation with k=5 for KNN\n",
    "knn = KNN(n_neighbors=5)\n",
    "scores = cross_val_score(knn, X, y, cv=5, scoring='accuracy')\n",
    "print(scores)\n",
    "print(scores.shape)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for an optimal value of k for KNN\n",
    "k_range = range(1,31)\n",
    "k_scores = []\n",
    "for k in k_range:\n",
    "    knn = KNN(n_neighbors=k)\n",
    "    k_scores.append(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('K range')\n",
    "plt.ylabel('K scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.3.2 Cross-Validation example: model selection\n",
    "Note: Compare the best KNN model with logistic regression on the iris dataset  \n",
    "-> result: KNN has better score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNN(n_neighbors=20)\n",
    "print(cross_val_score(knn, X, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "Logreg = LogisticRegression()\n",
    "print(cross_val_score(Logreg, X, y, cv=10, scoring='accuracy').mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 4.3.3 Cross-Validation example: feature selection\n",
    "Note: Select whether the Newspaper feature should be inculded inthe linear regression model on the advertising dataset  \n",
    "-> result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "data = pd.read_csv(\"Dataset/Advertising.csv\",index_col=0)\n",
    "feature_cols = list(data.drop('Sales',axis=1).columns)\n",
    "X=data[feature_cols]\n",
    "y = data['Sales']\n",
    "ln = LinearRegression()\n",
    "score = cross_val_score(ln,X,y,cv=10,scoring='neg_mean_squared_error')\n",
    "print(np.sqrt(-score))\n",
    "print(np.sqrt(-score).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = list(data.drop(['Newspaper','Sales'],axis=1).columns)\n",
    "X = data[feature_cols]\n",
    "score = np.sqrt(-cross_val_score(ln, X, y, cv=10, scoring='neg_mean_squared_error').mean())\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from pandas import read_csv\n",
    "from sklearn.utils import resample\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "data = read_csv('./Datasets/pima-indians-diabetes.data', header=None)\n",
    "values = data.values\n",
    "# configure bootstrap\n",
    "n_iterations = 1000\n",
    "n_size = int(len(data) * 0.50)\n",
    "# run bootstrap\n",
    "stats = list()\n",
    "for i in range(n_iterations):\n",
    "    # prepare train and test sets\n",
    "    train = resample(values, n_samples=n_size)\n",
    "    test = numpy.array([x for x in values if x.tolist() not in train.tolist()])\n",
    "    # fit model\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(train[:,:-1], train[:,-1])\n",
    "    # evaluate model\n",
    "    predictions = model.predict(test[:,:-1])\n",
    "    score = accuracy_score(test[:,-1], predictions)\n",
    "    stats.append(score)\n",
    "# plot scores\n",
    "pyplot.hist(stats)\n",
    "pyplot.show()\n",
    "# confidence intervals\n",
    "alpha = 0.95\n",
    "p = ((1.0-alpha)/2.0) * 100\n",
    "lower = max(0.0, numpy.percentile(stats, p))\n",
    "p = (alpha+((1.0-alpha)/2.0)) * 100\n",
    "upper = min(1.0, numpy.percentile(stats, p))\n",
    "print('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
